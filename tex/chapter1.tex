\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{array}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{titlesec}

% Define new environments for examples and solutions
\newtheorem{example}{Example}
\newenvironment{solution}{\noindent\textbf{Solution:}}{\hfill$\square$}

% Page settings
\geometry{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=20mm,
}

% Header and Footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Stochastic Processes Lecture Notes}
\fancyhead[R]{Chapter 1 - Fall 2024}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\href{https://stoch-sut.github.io/}{Course Website}}

% Title formatting
\title{
    \vspace{-2cm}
    \LARGE{Stochastic Processes} \\
    \vspace{0.5cm}
    \Large{Lecture Notes} \\
    \vspace{0.5cm}
    \normalsize{Chapter 1: Introduction to Random Variables}
}
\author{
    Payam Taebi \\
    \vspace{0.2cm}
}
\date{
    Fall 2024 \\
    \vspace{0.2cm}
    \href{https://stoch-sut.github.io/}{https://stoch-sut.github.io/}
}


% Section formatting for better readability
\titleformat{\section}
  {\Large\bfseries}{\thesection}{1em}{}

\titleformat{\subsection}
  {\large\bfseries}{\thesubsection}{1em}{}

\titleformat{\subsubsection}
  {\bfseries}{\thesubsubsection}{1em}{}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction}
Welcome to Chapter 1 of the Stochastic Processes course. This chapter introduces the fundamental concepts of random variables, their distributions, and essential properties that form the basis for understanding more complex stochastic processes. We will explore both discrete and continuous random variables, delve into probability mass and density functions, and examine key theorems that underpin probability theory.

\section{Random Variables (RV)}
\subsection{Definition}
A random variable \( X \) is a function \( X: \Omega \rightarrow \mathbb{R} \) where \( \Omega \) is the sample space. It assigns a real number to each outcome in the sample space, allowing us to quantify uncertainty.

\subsection{Types of Random Variables}
\begin{itemize}
    \item \textbf{Discrete RV}: Takes countable values (e.g., the outcome of a dice roll).
        \begin{itemize}
            \item \textbf{Probability Mass Function (PMF)}: \( P(X = x_i) = p(x_i) \)
        \end{itemize}
    \item \textbf{Continuous RV}: Takes uncountable values within an interval (e.g., height of individuals).
        \begin{itemize}
            \item \textbf{Probability Density Function (PDF)}: \( f_X(x) \)
        \end{itemize}
    \item \textbf{Real vs. Complex RV}:
        \begin{itemize}
            \item \textbf{Real}: \( X: \Omega \rightarrow \mathbb{R} \)
            \item \textbf{Complex}: \( X: \Omega \rightarrow \mathbb{C} \)
        \end{itemize}
\end{itemize}

\section{Density and Distribution Functions}
\subsection{Probability Mass Function (PMF)}
\textbf{Discrete RV:}
\[
P(X = x_i) = p(x_i)
\]

\textbf{Example: Fair Dice}
\begin{example}
For a fair six-sided die, the PMF is:
\[
p(x) = \frac{1}{6}, \quad x = 1, 2, 3, 4, 5, 6
\]
\end{example}
\begin{solution}
Each outcome from 1 to 6 is equally likely, hence the probability for each outcome is \( \frac{1}{6} \).
\end{solution}

\subsection{Probability Density Function (PDF)}
\textbf{Continuous RV:}
\[
P(a \leq X \leq b) = \int_a^b f_X(x) \, dx
\]

\textbf{Common PDFs:}
\begin{itemize}
    \item \textbf{Uniform Distribution}:
    \[
    f_X(x) = 
    \begin{cases} 
    \frac{1}{U - L} & L \leq x \leq U \\
    0 & \text{otherwise}
    \end{cases}
    \]
    \item \textbf{Gaussian (Normal) Distribution}:
    \[
    f_X(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{ -\frac{(x - \mu)^2}{2\sigma^2} }
    \]
    \item \textbf{Binomial Distribution}:
    \[
    P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}, \quad k = 0,1,\ldots,n
    \]
    \item \textbf{Poisson Distribution}:
    \[
    P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k = 0,1,2,\ldots
    \]
    \item \textbf{Exponential Distribution}:
    \[
    f_X(x) = 
    \begin{cases} 
    \lambda e^{-\lambda x} & x \geq 0 \\
    0 & \text{otherwise}
    \end{cases}
    \]
\end{itemize}

\textbf{Example: Exponential Distribution}
\begin{example}
Let \( X \) be an exponential random variable with rate parameter \( \lambda = 2 \). Find \( P(X \leq 1) \).
\end{example}
\begin{solution}
Using the Cumulative Distribution Function (CDF) of the exponential distribution:
\[
P(X \leq 1) = 1 - e^{-\lambda \cdot 1} = 1 - e^{-2} \approx 0.8647
\]
\end{solution}

\subsection{Cumulative Distribution Function (CDF)}
\textbf{Definition:}
\[
F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(t) \, dt
\]

\textbf{Properties:}
\begin{itemize}
    \item Non-decreasing
    \item Right-continuous
    \item \( \lim_{x \to -\infty} F_X(x) = 0 \)
    \item \( \lim_{x \to \infty} F_X(x) = 1 \)
\end{itemize}

\section{Expected Value and Moments}
\subsection{Expectation}
\textbf{Standard Form:}
\[
E[X] = \int_{-\infty}^{\infty} x f_X(x) \, dx \quad (\text{Continuous})
\]
\[
E[X] = \sum_{i} x_i p(x_i) \quad (\text{Discrete})
\]

\textbf{Alternative Forms:}

\textbf{For Non-Negative Random Variables (\( X \geq 0 \)):}
\[
E[X] = \int_0^{\infty} P(X > a) \, da
\]

\textbf{General Form (Allowing \( X \) to Take Negative Values):}
\[
E[X] = \int_{-\infty}^0 P(X < a) \, da + \int_0^{\infty} P(X > a) \, da
\]

\textbf{Explanation:}
\begin{itemize}
    \item This representation decomposes the expected value into contributions from the lower and upper tails of the distribution.
    \item It is particularly useful in scenarios where evaluating \( E[X] \) directly is challenging, but tail probabilities \( P(X > a) \) and \( P(X < a) \) are easier to compute or estimate.
\end{itemize}

\textbf{Example: Expected Value Using Tail Probabilities}
\begin{example}
Let \( X \) be a non-negative continuous random variable with PDF \( f_X(x) = \lambda e^{-\lambda x} \) for \( x \geq 0 \) (Exponential Distribution). Compute \( E[X] \) using the tail probability method.
\end{example}
\begin{solution}
Using the alternative form for non-negative random variables:
\[
E[X] = \int_0^{\infty} P(X > a) \, da = \int_0^{\infty} e^{-\lambda a} \, da = \frac{1}{\lambda}
\]
For \( \lambda = 2 \):
\[
E[X] = \frac{1}{2}
\]
\end{solution}

\subsection{Linearity of Expectation}
\[
E[aX + b] = aE[X] + b
\]

\subsection{Higher Moments}
\[
M_n = E[X^n]
\]

\subsection{Variance}
\[
\text{Var}(X) = E[X^2] - (E[X])^2
\]

\subsection{Function of RV}
\[
E[g(X)] = \int_{-\infty}^{\infty} g(x) f_X(x) \, dx
\]

\textbf{Example: Expectation of a Function}
\begin{example}
Let \( X \sim \mathcal{N}(0, 1) \). Compute \( E[X^2] \).
\end{example}
\begin{solution}
Since \( X \) is a standard normal random variable:
\[
E[X^2] = \text{Var}(X) + (E[X])^2 = 1 + 0 = 1
\]
\end{solution}

\section{Joint and Conditional Distributions}
\subsection{Joint Probability Functions}
\textbf{Joint PDF/PMF:}
\[
f_{X,Y}(x,y) = P(X=x, Y=y) \quad (\text{Discrete})
\]
\[
f_{X,Y}(x,y) = \frac{\partial^2 F_{X,Y}(x,y)}{\partial x \partial y} \quad (\text{Continuous})
\]

\subsection{Marginal Distributions}
\textbf{From Joint PDF:}
\[
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dy
\]
\[
f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dx
\]

\subsection{Conditional Distributions}
\textbf{Definition:}
\[
f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}
\]
\[
f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}
\]

\textbf{Bayesâ€™ Theorem:}
\[
f_{Y|X}(y|x) = \frac{f_X(x|y) f_Y(y)}{f_X(x)}
\]

\subsection{Independence}
\textbf{Condition:}
\[
X \text{ and } Y \text{ are independent } \iff f_{X,Y}(x,y) = f_X(x) f_Y(y)
\]

\subsection{Transformation of Variables}
\textbf{Function of Joint RVs:}
\[
f_{U,V}(u,v) = f_{X,Y}(x(u,v), y(u,v)) \left| \det J \right|
\]
where \( J \) is the Jacobian matrix:
\[
J = 
\begin{pmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{pmatrix}
\]

\textbf{Example: Transformation of Variables}
\begin{example}
Let \( U = X + Y \) and \( V = X - Y \), where \( X \) and \( Y \) are independent standard normal random variables. Find the joint PDF of \( U \) and \( V \).
\end{example}
\begin{solution}
First, express \( X \) and \( Y \) in terms of \( U \) and \( V \):
\[
X = \frac{U + V}{2}, \quad Y = \frac{U - V}{2}
\]
Compute the Jacobian determinant:
\[
J = 
\begin{pmatrix}
\frac{\partial X}{\partial U} & \frac{\partial X}{\partial V} \\
\frac{\partial Y}{\partial U} & \frac{\partial Y}{\partial V}
\end{pmatrix}
= 
\begin{pmatrix}
\frac{1}{2} & \frac{1}{2} \\
\frac{1}{2} & -\frac{1}{2}
\end{pmatrix}
\]
\[
\det J = \left(\frac{1}{2}\right)\left(-\frac{1}{2}\right) - \left(\frac{1}{2}\right)\left(\frac{1}{2}\right) = -\frac{1}{4} - \frac{1}{4} = -\frac{1}{2}
\]
Taking absolute value:
\[
\left| \det J \right| = \frac{1}{2}
\]
Since \( X \) and \( Y \) are independent standard normals:
\[
f_{X,Y}(x,y) = \frac{1}{2\pi} e^{-\frac{x^2 + y^2}{2}}
\]
Thus, the joint PDF of \( U \) and \( V \) is:
\[
f_{U,V}(u,v) = f_{X,Y}\left(\frac{u+v}{2}, \frac{u-v}{2}\right) \cdot \frac{1}{2} = \frac{1}{2\pi} e^{-\frac{\left(\frac{u+v}{2}\right)^2 + \left(\frac{u-v}{2}\right)^2}{2}} \cdot \frac{1}{2} = \frac{1}{4\pi} e^{-\frac{u^2 + v^2}{4}}
\]
\end{solution}

\section{Linear Correlation}
\subsection{Covariance}
\textbf{Definition:}
\[
\text{Cov}(X,Y) = E[XY] - E[X]E[Y]
\]

\subsection{Correlation Coefficient}
\textbf{Definition:}
\[
\rho_{XY} = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}
\]
where \( \sigma_X = \sqrt{\text{Var}(X)} \) and \( \sigma_Y = \sqrt{\text{Var}(Y)} \)

\subsection{Properties}
\begin{itemize}
    \item \( \rho_{XY} = 1 \) : Perfect positive linear relationship
    \item \( \rho_{XY} = -1 \) : Perfect negative linear relationship
    \item \( \rho_{XY} = 0 \) : No linear relationship (not necessarily independent)
\end{itemize}

\textbf{Example: Correlation Calculation}
\begin{example}
Let \( X \) and \( Y \) be two random variables with:
\[
E[X] = 2, \quad E[Y] = 3, \quad E[XY] = 11, \quad \text{Var}(X) = 4, \quad \text{Var}(Y) = 9
\]
Compute the correlation coefficient \( \rho_{XY} \).
\end{example}
\begin{solution}
First, compute the covariance:
\[
\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = 11 - (2)(3) = 5
\]
Next, compute the standard deviations:
\[
\sigma_X = \sqrt{4} = 2, \quad \sigma_Y = \sqrt{9} = 3
\]
Finally, compute the correlation coefficient:
\[
\rho_{XY} = \frac{5}{(2)(3)} = \frac{5}{6} \approx 0.8333
\]
\end{solution}

\section{Important Theorems}
\subsection{Central Limit Theorem (CLT)}
\textbf{Statement:}
\begin{itemize}
    \item Let \( X_1, X_2, \ldots, X_n \) be i.i.d. RVs with \( E[X_i] = \mu \) and \( \text{Var}(X_i) = \sigma^2 \).
    \item Define \( S_n = \sum_{i=1}^n X_i \).
    \item Then, as \( n \to \infty \),
    \[
    \frac{S_n - n\mu}{\sigma \sqrt{n}} \xrightarrow{d} \mathcal{N}(0,1)
    \]
\end{itemize}

\textbf{Exception:}
\begin{itemize}
    \item Cauchy Distribution does not satisfy CLT conditions.
\end{itemize}

\textbf{Example: CLT Application}
\begin{example}
Let \( X_1, X_2, \ldots, X_{100} \) be i.i.d. exponential random variables with \( \lambda = 1 \). Approximate the distribution of \( S_{100} = \sum_{i=1}^{100} X_i \) using CLT.
\end{example}
\begin{solution}
Each \( X_i \) has \( E[X_i] = 1 \) and \( \text{Var}(X_i) = 1 \). By CLT:
\[
\frac{S_{100} - 100}{10} \xrightarrow{d} \mathcal{N}(0,1)
\]
Thus, \( S_{100} \) is approximately normally distributed with mean \( 100 \) and variance \( 100 \), i.e., \( S_{100} \sim \mathcal{N}(100, 100) \).
\end{solution}

\subsection{Law of Large Numbers (LLN)}
\textbf{Weak Law:}
\[
\frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{P} \mu \quad \text{as } n \to \infty
\]

\textbf{Strong Law:}
\[
\frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{\text{a.s.}} \mu \quad \text{as } n \to \infty
\]

\subsection{Chebyshevâ€™s Inequality}
\textbf{Statement:}
\[
P(|X - E[X]| \geq k\sigma) \leq \frac{1}{k^2}, \quad k > 0
\]

\textbf{Alternative Forms:}
\begin{itemize}
    \item For any \( \epsilon > 0 \):
    \[
    P(|X - \mu| \geq \epsilon) \leq \frac{\text{Var}(X)}{\epsilon^2}
    \]
\end{itemize}

\textbf{Example: Chebyshevâ€™s Inequality Application}
\begin{example}
Let \( X \) be a random variable with \( E[X] = 10 \) and \( \text{Var}(X) = 25 \). Find an upper bound for \( P(|X - 10| \geq 10) \) using Chebyshevâ€™s inequality.
\end{example}
\begin{solution}
Using Chebyshevâ€™s inequality:
\[
P(|X - 10| \geq 10) \leq \frac{25}{10^2} = \frac{25}{100} = 0.25
\]
\end{solution}

\subsection{Cauchy-Schwarz Inequality}
\textbf{Statement:}
\[
|E[XY]| \leq \sqrt{E[X^2] E[Y^2]}
\]

\textbf{Equality Condition:}
\begin{itemize}
    \item Occurs if and only if \( X \) and \( Y \) are linearly dependent.
\end{itemize}

\textbf{Example: Cauchy-Schwarz Inequality}
\begin{example}
Let \( X \) and \( Y \) be random variables with \( E[X] = E[Y] = 0 \), \( E[X^2] = 4 \), \( E[Y^2] = 9 \), and \( E[XY] = 6 \). Verify the Cauchy-Schwarz inequality and determine if equality holds.
\end{example}
\begin{solution}
Compute \( |E[XY]| \) and \( \sqrt{E[X^2] E[Y^2]} \):
\[
|E[XY]| = |6| = 6
\]
\[
\sqrt{E[X^2] E[Y^2]} = \sqrt{4 \times 9} = \sqrt{36} = 6
\]
Since \( |E[XY]| = \sqrt{E[X^2] E[Y^2]} \), equality holds, implying that \( X \) and \( Y \) are linearly dependent.
\end{solution}

\section{Stochastic Processes}
\subsection{Definition}
A stochastic process \( \{X(t) | t \in T\} \) is a collection of random variables indexed by time \( t \), where \( T \) can be discrete or continuous. It models systems or phenomena that evolve over time under uncertainty.

\subsection{Examples}
\begin{itemize}
    \item Random Walk
    \item Poisson Process
    \item Gaussian Process
    \item Stock Prices
\end{itemize}

\subsection{Properties}
\begin{itemize}
    \item \textbf{Stationarity}:
    \begin{itemize}
        \item \textbf{Strict Sense}: All finite-dimensional distributions are invariant under time shifts.
        \item \textbf{Wide Sense (Weak Stationarity)}: 
        \begin{itemize}
            \item \( E[X(t)] = \mu \) (constant)
            \item \( \text{Cov}(X(t), X(t+\tau)) \) depends only on \( \tau \).
        \end{itemize}
    \end{itemize}
    \item \textbf{Ergodicity}: Time averages equal ensemble averages.
    \item \textbf{Markov Property}: Future states depend only on the present state, not on the history.
\end{itemize}

\subsection{Mean and Variance Functions}
\textbf{Mean:}
\[
m(t) = E[X(t)]
\]

\textbf{Variance:}
\[
\text{Var}(X(t)) = E[X(t)^2] - (E[X(t)])^2
\]

\subsection{Joint Distributions}
\textbf{For \( X(t_1), X(t_2), \ldots, X(t_n) \):}
\[
f_{X(t_1), X(t_2), \ldots, X(t_n)}(x_1, x_2, \ldots, x_n)
\]

\subsection{Examples of Stochastic Processes}

\subsubsection{Example 1: Linear Stochastic Process}
\begin{example}
Consider the stochastic process:
\[
X(t) = A t + b
\]
where:
\begin{itemize}
    \item \( A \) is a Gaussian RV, \( A \sim \mathcal{N}(0,1) \)
    \item \( b \) is a constant
\end{itemize}
Find the PDF of \( X(t) \), and compute its mean and variance.
\end{example}
\begin{solution}
Since \( A \sim \mathcal{N}(0,1) \), \( X(t) \) is also normally distributed:
\[
X(t) \sim \mathcal{N}(b, t^2)
\]
Thus, the PDF of \( X(t) \) is:
\[
f_X(x,t) = \frac{1}{\sqrt{2\pi t^2}} \exp\left( -\frac{(x - b)^2}{2 t^2} \right)
\]
Mean and Variance:
\[
E[X(t)] = b
\]
\[
\text{Var}(X(t)) = t^2
\]
\end{solution}

\subsubsection{Example 2: Oscillatory Stochastic Process}
\begin{example}
Consider the stochastic process:
\[
X(t) = a \cos(\omega_0 t) + \phi
\]
where:
\begin{itemize}
    \item \( a \) is a constant
    \item \( \phi \) is uniformly distributed in \( (0, 2\pi] \)
\end{itemize}
Find the PDF of \( X(t) \), and compute its mean and variance.
\end{example}
\begin{solution}
Since \( \phi \) is uniformly distributed in \( (0, 2\pi] \), \( X(t) \) oscillates between \( a \cos(\omega_0 t) \) and \( a \cos(\omega_0 t) + 2\pi \).

Assuming \( \phi \) is uniformly distributed over \( (0, 2\pi) \) and considering the distribution of \( X(t) \):
\[
f_X(x,t) = \frac{1}{2\pi} \quad \text{for } x \in (a \cos(\omega_0 t), a \cos(\omega_0 t) + 2\pi)
\]
Mean and Variance:
\[
E[X(t)] = a \cos(\omega_0 t) + E\{\phi\} = a \cos(\omega_0 t) + \pi
\]
\[
\text{Var}\{X(t)\} = \text{Var}\{\phi\} = \frac{(2\pi)^2}{12} = \frac{\pi^2}{3}
\]
\end{solution}

\section{Additional Formulas}
\subsection{Moment Generating Function (MGF)}
\textbf{Definition:}
\[
M_X(t) = E[e^{tX}]
\]

\textbf{Properties:}
\begin{itemize}
    \item \textbf{Linearity}: For constants \( a, b \), 
    \[
    M_{aX + b}(t) = e^{bt} M_X(at)
    \]
    \item \textbf{Independence}: If \( X \) and \( Y \) are independent, 
    \[
    M_{X+Y}(t) = M_X(t) M_Y(t)
    \]
    \item \textbf{Derivatives}: The \( n \)-th derivative of \( M_X(t) \) evaluated at \( t=0 \) gives the \( n \)-th moment:
    \[
    M_X^{(n)}(0) = E[X^n]
    \]
\end{itemize}

\subsection{Characteristic Function}
\textbf{Definition:}
\[
\phi_X(\omega) = E[e^{i\omega X}]
\]
where \( i \) is the imaginary unit and \( \omega \) is a real number.

\textbf{Properties:}
\begin{itemize}
    \item \textbf{Always Exists}: For any RV \( X \), the characteristic function exists.
    \item \textbf{Uniqueness}: The characteristic function uniquely determines the distribution of \( X \).
    \item \textbf{Independence}: If \( X \) and \( Y \) are independent, 
    \[
    \phi_{X+Y}(\omega) = \phi_X(\omega) \phi_Y(\omega)
    \]
    \item \textbf{Relation to MGF}: 
    \[
    \phi_X(\omega) = M_X(i\omega)
    \]
\end{itemize}

\subsubsection{Characteristic Function of Normal Distribution}
\begin{example}
For \( X \sim \mathcal{N}(\mu, \sigma^2) \), find the characteristic function \( \phi_X(\omega) \).
\end{example}
\begin{solution}
The characteristic function of a normal distribution is:
\[
\phi_X(\omega) = e^{i\omega\mu - \frac{1}{2} \omega^2 \sigma^2}
\]
This formula shows that the characteristic function of a normal distribution is an exponential function involving the mean \( \mu \) and variance \( \sigma^2 \).
\end{solution}

\subsection{Generating Functions Comparison}
\textbf{MGF vs. Characteristic Function:}
\begin{itemize}
    \item \textbf{MGF}:
    \[
    M_X(t) = E[e^{tX}]
    \]
    \begin{itemize}
        \item Defined for real \( t \).
        \item May not exist for all \( t \) depending on the distribution.
    \end{itemize}
    \item \textbf{Characteristic Function}:
    \[
    \phi_X(\omega) = E[e^{i\omega X}]
    \]
    \begin{itemize}
        \item Defined for all real \( \omega \).
        \item Always exists for any RV \( X \).
    \end{itemize}
\end{itemize}

\textbf{Relation Between MGF and Characteristic Function:}
\[
\phi_X(\omega) = M_X(i\omega)
\]
This shows that the characteristic function is essentially the MGF evaluated at an imaginary argument.

\subsection{Covariance Matrix (for Multivariate RVs)}
\textbf{Definition:}
\[
\Sigma = 
\begin{pmatrix}
\text{Var}(X) & \text{Cov}(X,Y) \\
\text{Cov}(Y,X) & \text{Var}(Y)
\end{pmatrix}
\]

\subsection{Transformation of a Single RV}
\textbf{If \( Y = g(X) \), and \( g \) is monotonic:}
\[
f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|
\]

\section{Key Concepts}
\subsection{Stationary Process}
\begin{itemize}
    \item \textbf{Strict Stationarity}: All joint distributions are invariant under time shifts.
    \item \textbf{Wide Sense Stationarity}: Mean is constant and covariance depends only on time difference.
\end{itemize}

\subsection{Ergodicity}
Time averages equal ensemble averages.

\subsection{Markov Property}
Future states depend only on the present state, not on the history.

\section{Quick Reference: Common Distributions}
\begin{center}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Distribution} & \textbf{PMF/PDF} & \textbf{Parameters} \\ \midrule
\textbf{Uniform} & \( f_X(x) = \frac{1}{U - L} \) for \( L \leq x \leq U \) & Lower \( L \), Upper \( U \) \\
\textbf{Gaussian} & \( f_X(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{ -\frac{(x - \mu)^2}{2\sigma^2} } \) & Mean \( \mu \), Std Dev \( \sigma \) \\
\textbf{Binomial} & \( P(X = k) = \binom{n}{k} p^k (1-p)^{n-k} \) & Trials \( n \), Success \( p \) \\
\textbf{Poisson} & \( P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!} \) & Rate \( \lambda \) \\
\textbf{Exponential} & \( f_X(x) = \lambda e^{-\lambda x} \) for \( x \geq 0 \) & Rate \( \lambda \) \\ \bottomrule
\end{tabular}
\end{center}

\textbf{Example: Quick Reference Usage}
\begin{example}
Identify the distribution and its parameters for the following scenario:
A machine produces widgets with the number of defects following a Poisson distribution with an average rate of 3 defects per hour.
\end{example}
\begin{solution}
The number of defects follows a Poisson distribution with parameter \( \lambda = 3 \).
\end{solution}

\section{Conclusion}
These lecture notes cover the fundamental concepts of Chapter 1 in Stochastic Processes, including random variables, probability distributions, expectation, covariance, important theorems, and an introduction to stochastic processes. For detailed explanations and further readings, please refer to your course materials or standard textbooks.

\end{document}
